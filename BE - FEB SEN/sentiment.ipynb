{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25386,"status":"ok","timestamp":1680681742577,"user":{"displayName":"sandalu sankaja de silva","userId":"12826508473947062911"},"user_tz":-330},"id":"ikwoxhyMekTu","outputId":"f74f1f80-2b01-4ee0-b315-b43d0fe6ced1"},"outputs":[],"source":["!pip install tensorflow\n","!pip install nltk\n","!pip install snscrape\n","!pip install pyngrok\n","!pip install flask-ngrok"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2837,"status":"ok","timestamp":1680674636011,"user":{"displayName":"sandalu sankaja de silva","userId":"12826508473947062911"},"user_tz":-330},"id":"vXzBgCLhQHgg","outputId":"4df90c65-1c4f-48c8-d698-aa572e060377"},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('/content/drive')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5kizkOoPQhrp"},"outputs":[],"source":["#cd /content/drive/MyDrive/IndexPage"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5TrshOseUn_o"},"outputs":[],"source":["from flask_ngrok import run_with_ngrok\n","from flask import Flask, render_template, request\n","\n","\n","import tensorflow as tf\n","\n","\n","import pandas as pd \n","import datetime\n","from tqdm.notebook import tqdm\n","import snscrape.modules.twitter as sntwitter\n","\n","\n","app = Flask(__name__)\n","\n","\n","from pyngrok import ngrok\n","ngrok.set_auth_token('2NpJgVAPvnKxciFH5fjre1ZkRLl_6sr3HQirtTuoAciZZeoY3')\n","\n","# Now you can use ngrok.connect() to expose a local web server\n","\n","import re\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from bs4 import BeautifulSoup\n","\n","# removing not and no from stop words as we need them for our model training\n","stopwords = stopwords.words(\"english\")\n","stopwords.remove('not')\n","stopwords.remove('no')\n","\n","# intializing method for lemmatizing words\n","lemmatizer = WordNetLemmatizer()\n","\n","# now creating funtion to clean our data\n","def cleaned_review(review):\n","    # remove any html tags\n","    new_review = BeautifulSoup(review).get_text()\n","    \n","    # remove urls from reviews\n","    no_urls = new_review.replace('http\\S+', '').replace('www\\S+', '')\n","    \n","    # remove any non-letters\n","    clean_review = re.sub(\"[^a-zA-Z]\", \" \", no_urls)\n","    \n","    # convert whole sentence to lowercase and split\n","    new_words = clean_review.lower().split()\n","    \n","    # converting stopwords list to set for faster search\n","    stops = set(stopwords)\n","    \n","    # using stopwords to remove irrelavent words and lemmatizing the final output\n","    final_words = [lemmatizer.lemmatize(word) for word in new_words if not word in stops]\n","    \n","    # return the final result\n","    return (\" \".join(final_words))\n","\n","# Importing required libraries\n","from tensorflow.keras.models import load_model\n","import numpy as np\n","\n","# Loading the saved model\n","newModel = load_model('ModelLSDM')\n","newModel.summary()\n","\n","\n","run_with_ngrok(app)\n","import os\n","\n","@app.route('/')\n","def index():\n","   return render_template(\"index.html\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7spKJ7arUsAs"},"outputs":[],"source":["@app.route('/results', methods=['POST'])\n","def results():\n","\n","    profile = request.form['username']\n","    print(profile)\n","\n","    # Scrape Twitter profile\n","    now = datetime.datetime.now()\n","    twenty_four_hours_ago = now - datetime.timedelta(hours=24)\n","    formatted_time = twenty_four_hours_ago.strftime('%Y-%m-%d')\n","    userScraper = sntwitter.TwitterProfileScraper(profile)\n","    userdata = userScraper._get_entity()\n","\n","    \n","    tweets = []\n","    input_text=\"\"\n","    for i, tweet in enumerate(userScraper.get_items()):\n","        if isinstance(tweet, sntwitter.Tweet) and tweet.date.strftime('%Y-%m-%d') >= formatted_time:\n","            if(i > 1):\n","                break\n","            data = [tweet.date,\n","                    tweet.rawContent,\n","                    tweet.likeCount,\n","                    tweet.retweetCount]\n","            input_text += tweet.rawContent\n","            tweets.append(data)\n","\n","    input_text = cleaned_review(input_text)\n","\n","    # So we see above that max no.of words in each sentence is 40\n","    max_len = 40\n","\n","    from tensorflow.keras.preprocessing.text import Tokenizer\n","    from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","    tokenizer = Tokenizer(num_words=10000)\n","    tokenizer.fit_on_texts(input_text)\n","\n","    # # Preprocess input text\n","    input_sequence = tokenizer.texts_to_sequences([input_text])\n","    padded_input_sequence = pad_sequences(input_sequence, maxlen=max_len, padding='post')\n","    \n","    # Make prediction\n","    preds = newModel.predict(padded_input_sequence)\n","    sentiment_labels = ['Negative', 'Neutral', 'Positive']\n","    predicted_label = sentiment_labels[np.argmax(preds)]\n","\n","    return render_template('index.html', predicted_label=predicted_label)\n","\n","    #return render_template('results.html', predicted_label=predicted_label)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"tzcuPMJQdGyB"},"outputs":[],"source":["app.run()"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}
